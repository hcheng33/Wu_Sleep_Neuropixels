{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows Clark WS Path\n",
    "#spike_times = \"Z:/Wu_sleep/m1/SD1/20240914_SD1_test_1200_g0/catgt_20240914_SD1_test_1200_g0/20240914_SD1_test_1200_g0_imec0/imec0_ks25/spike_times.npy\"\n",
    "#spike_clusters = \"Z:/Wu_sleep/m1/SD1/20240914_SD1_test_1200_g0/catgt_20240914_SD1_test_1200_g0/20240914_SD1_test_1200_g0_imec0/imec0_ks25/spike_clusters.npy\"\n",
    "\n",
    "# mac macAir Path\n",
    "file_dir = \"/Users/alexandracheng/Desktop/Harris_Lab/Collab/Wu_Sleep/Wu_Sleep_Data/\"\n",
    "#spike_times_file = \"/Users/alexandracheng/Desktop/Harris_Lab/Collab/Wu_Sleep/Wu_Sleep_Data/spike_times_m1_1200.npy\"\n",
    "#spike_clusters_file = \"/Users/alexandracheng/Desktop/Harris_Lab/Collab/Wu_Sleep/Wu_Sleep_Data/spike_clusters_m1_1200.npy\"\n",
    "\n",
    "# parameters\n",
    "bin_size = 15000\n",
    "\n",
    "sample_rate = 30000\n",
    "sigma = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def firing_rate(spike_times_file, spike_clusters_file, bin_size, cluster):\n",
    "    spike_times = np.load(spike_times_file)\n",
    "    spike_clusters = np.load(spike_clusters_file)\n",
    "\n",
    "    if cluster == \"all\":\n",
    "        clust_ind = np.unique(spike_clusters)\n",
    "    else:\n",
    "        clust_ind = cluster\n",
    "    t_end = np.max(spike_times)\n",
    "    t_bins = np.arange(0,t_end,bin_size)\n",
    "    \n",
    "    clust_num = len(clust_ind)\n",
    "    fr = np.zeros((len(clust_ind), len(t_bins)-1))\n",
    "    for i in range(len(clust_ind)):\n",
    "        spikes_t_ind = spike_times[np.where(spike_clusters == clust_ind[i])[0]]\n",
    "        spikes_count, edges = np.histogram(spikes_t_ind, t_bins)\n",
    "\n",
    "        fr[i,:] = spikes_count\n",
    "\n",
    "    return fr, t_bins[:-1], clust_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def firing_rate_smooth(fr, sigma):\n",
    "    fr_smooth = np.zeros(fr.shape)\n",
    "    for i in range(len(fr_smooth)):\n",
    "        fr_smooth[i,:] = gaussian_filter1d(fr[i,:], sigma)\n",
    "\n",
    "    return fr_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr, t_bins, clust_num = firing_rate(spike_times_file, spike_clusters_file, bin_size, \"all\")\n",
    "fr_smooth = firing_rate_smooth(fr, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stamp = np.arange(900, 2200, 100)\n",
    "#file_name = \"spike_times\"+\"_m1_\"+str(t_stamp[0])+\".npy\"\n",
    "\n",
    "mean_fr_sessions = []\n",
    "\n",
    "for s in range(len(t_stamp)):\n",
    "    spike_times_file = file_dir + \"spike_times\"+\"_m1_\"+str(t_stamp[s])+\".npy\"\n",
    "    spike_clusters_file = file_dir + \"spike_clusters\"+\"_m1_\"+str(t_stamp[s])+\".npy\"\n",
    "\n",
    "    fr, t_bins, clust_num = firing_rate(spike_times_file, spike_clusters_file, bin_size, \"all\")\n",
    "    fr_smooth = firing_rate_smooth(fr, sigma)\n",
    "    fr_smooth_avg = np.average(fr_smooth, axis=1)\n",
    "\n",
    "    mean_fr_sessions.append(fr_smooth_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mean_fr_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_times_files = file_dir + \"spike_times\"+\"_m1_\"+str(t_stamp[2])+\".npy\"\n",
    "spike_clusters_file = file_dir + \"spike_clusters\"+\"_m1_\"+str(t_stamp[2])+\".npy\"\n",
    "\n",
    "fr, t_bins, clust_num = firing_rate(spike_times_file, spike_clusters_file, bin_size, \"all\")\n",
    "fr_smooth = firing_rate_smooth(fr, sigma)\n",
    "fr_smooth_avg = np.average(fr_smooth, axis=1)\n",
    "\n",
    "mean_fr_sessions[i,0] = clust_num\n",
    "mean_fr_sessions[i,1:clust_num+1] = fr_smooth_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (t_bins/sample_rate).reshape(-1,1)\n",
    "y = fr[241,:]\n",
    "\n",
    "gpr = GaussianProcessRegressor().fit(X, y)\n",
    "mean_prediction, std_prediction = gpr.predict(X, return_std=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (t_bins/sample_rate).reshape(-1,1)\n",
    "y = fr[241,:]\n",
    "\n",
    "\n",
    "y3 = gaussian_filter1d(y, 3)\n",
    "y6 = gaussian_filter1d(y, 10)\n",
    "\n",
    "plt.plot(y[0:3000])\n",
    "plt.plot(y3[0:3000])\n",
    "plt.plot(y6[0:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[0:3000],y[0:3000])\n",
    "plt.plot(X[0:3000],mean_prediction[0:3000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
